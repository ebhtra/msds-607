---
title: "Spotify Data Getters"
author: "Ethan Haley"
date: "4/24/2021"
output: html_document
---

## This notebook has the code used for acquiring and organizing playlist-centric data from Spotify's API. 

##### It's not meant to be executed as is, since the API calls take many hours to run, and require password authentication, and also take up a large amount of memory.  However, the final product of this code, namely the structured dataframes, is saved and accessible at the Github URL's shown at the end of each completed section. They were used to build a Neo4j database, using an accompanying Cypher script.

```{r setup, include=FALSE, warning=F}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(httr)
library(jsonlite)
library(glue)
library(countrycode)
```

```{r}
## spotify developer auth passwords
my_ID <- 'xxxxxx'
my_secret <- 'xxxxxxx'
```

```{r}
# This gets you an access token that's good for 1 hour
get_auth <- function(){
  response <- POST('https://accounts.spotify.com/api/token',
                 accept_json(),
                 authenticate(my_ID, my_secret),
                 body = list(grant_type='client_credentials'),
                 encode = 'form')
  paste('Bearer', content(response)$access_token)
}

```

### Get all categories

```{r}
auth <- get_auth()
baseUrl <- 'https://api.spotify.com/v1/'
cat_list_endpt <- 'browse/categories'

cat_url <- paste0(baseUrl, cat_list_endpt, '?limit=50')
cat_resp <- GET(cat_url, add_headers(Authorization=auth))
# Looks like that returned 50 out of 54 categories
cat2_url <- paste0(baseUrl, cat_list_endpt, '?offset=50&limit=50')
cat2_resp <- GET(cat2_url, add_headers(Authorization=auth))

cat_IDs <- c()
cat_names <- c()
for (i in content(cat_resp)$categories$items) {
  cat_IDs <- c(cat_IDs, i$id)
  cat_names <- c(cat_names, i$name)
}
for (i in content(cat2_resp)$categories$items) {
  cat_IDs <- c(cat_IDs, i$id)
  cat_names <- c(cat_names, i$name)
}
cats <- data.frame(ID=cat_IDs, name=cat_names)
cats <- unique(cats)
cats
## Write it out:
#write_csv(cats, 'spotifyCategories.csv')
## Read it back in from Github later:
# cats <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/spotifyCategories.csv')
```

#### Get all playlists from all countries 

```{r}
#auth <- get_auth()
# countrycode package has a list of the 2-letter country codes spotify uses
countries <- codelist$iso2c
countries <- countries[!is.na(countries)]

masterframe = data.frame('collab' = c(),
                   'description' = c(),
                   'ID' = c(),
                   'imageUrl' = c(),
                   'name' = c(),
                   'owner' = c(),
                   'ownerID' = c(),
                   'ownerType' = c(),
                   'tracks' = c(),
                   'country' = c(),
                   'genre' = c())
get_all_playlists <- function(dfr) {
  for (co in countries) {
    for (ca in c(cats$ID)) {
      r <- get_playlists_by_category_country(ca, co)
      if (r$status_code == 200) {
        cr <- content(r)
        playlists <- parse_playlists(cr)
        playlists$country <- rep(co, length(playlists$ID))
        playlists$genre <- rep(ca, length(playlists$ID))
        dfr <- rbind(dfr, playlists)
      }
    }
  }
  return(dfr)
}

parse_playlists <- function(playlists_content) {
  templist <- list('collab' = c(),
                   'description' = c(),
                   'ID' = c(),
                   'imageUrl' = c(),
                   'name' = c(),
                   'owner' = c(),
                   'ownerID' = c(),
                   'ownerType' = c(),
                   'tracks' = c()
                   )
  for (item in playlists_content$playlists$items) {
    templist$collab <- c(templist$collab, item$collaborative)
    templist$description <- c(templist$description, item$description)
    templist$ID <- c(templist$ID, item$id)
    templist$imageUrl <- c(templist$imageUrl, item$images[[1]]$url)
    templist$name <- c(templist$name, item$name)
    templist$owner <- c(templist$owner, item$owner$display_name)
    templist$ownerID <- c(templist$ownerID, item$owner$id)
    templist$ownerType <- c(templist$ownerType, item$owner$type)
    templist$tracks <- c(templist$tracks, item$tracks$total)
  }
  return(templist)
}
```

```{r}
masterframe <- get_all_playlists(masterframe)
tail(masterframe)
## save this part thru mid-Portugal, to avoid repeating
#write_csv(masterframe, 'masterPlaylist.csv')
## Read it back in from Github later:
# masterframe <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/masterPlaylist.csv')
```

```{r}
masterframe <- read.csv('masterPlaylist.csv')
dim(masterframe)
head(masterframe)
```
```{r}
masterframe %>%
  group_by(country) %>%
  summarise(count = n()) %>%
  arrange(count) 
```
```{r}
# Portugal got partly duped from restarting the loop
masterframe <- masterframe[!duplicated(masterframe), ]
masterframe %>%
  group_by(country) %>%
  summarise(count = n()) %>%
  arrange(-count) 
```
Sorry, Portugal.

```{r}
# Convert 2-letter isocodes to country names
code2country <- function(code) {
  codelist$country.name.en[which(codelist$iso2c == code)]
}
code2country('IE')
```

```{r}
masterframe %>%
  group_by(ID) %>%
  summarise(count = n()) %>%
  arrange(-count) 
```



```{r}
auth <- get_auth()
get_playlists_by_category_country <- function(categoryID, countryCode='US', offset=0) {
  endpt <- glue(baseUrl, 'browse/categories/{categoryID}/playlists?country={countryCode}&offset={offset}&limit=50')
  resp <- GET(endpt, add_headers(Authorization=auth))
}
r <- get_playlists_by_category_country('toplists', 'ES')
content(r)
r2 <- get_playlists_by_category_country('toplists', 'ES')
content(r2)
content(r)

```

How many unique playlists globally?

```{r}
length(unique(masterframe$ID))
```

#### Get all songs from each playlist ID  

```{r}
auth <- get_auth()
baseUrl <- 'https://api.spotify.com/v1/'
pl_endpt <- 'playlists/'
```

```{r}
get_playlist_details <- function(playlistID) {
  pl_url <- paste0(baseUrl, pl_endpt, playlistID)
  pl_resp <- GET(pl_url, add_headers(Authorization=auth))
  return(list(pl_resp))
}
```

```{r}
auth <- get_auth()
resps <- c()
uID <- unique(masterframe$ID)
pl <- uID[3226:length(uID)]
for (p in pl) {
  r <- get_playlist_details(p)
  if (r[[1]]$status_code == 401) {
    cat("STUCK ON ", p)
    # print out the index that wasn't completed, and refresh auth
    get_auth()
  }
  resps <- c(resps, r)
}
# This won't be read back in at any point, but I'm storing it for possible future uses:
#save(resps, file='resps.RData')
resps[5813]
# clear space
resps <- 0
```

#### Parse the info out of the playlist responses  

```{r}
# collect track info in case it saves API calls later
allTracks <- c()

dates <- c()
follows <- c()
names <- c()
description <- c()
collaborative <- c()
ids <- c()
public <- c()
owner <- c()
numtracks <- c()
trackIDs <- list()

for (r in 1:length(resps)) {
  dates <- c(dates, resps[[r]]$date)
  
  cont <- content(resps[[r]])
  
  ids <- c(ids, cont$id)
  follows <- c(follows, cont$followers$total)
  collaborative <- c(collaborative, cont$collaborative)
  description <- c(description, cont$description)
  names <- c(names, cont$name)
  owner <- c(owner, cont$owner$id)
  public <- c(public, cont$public)
  numtracks <- c(numtracks, cont$tracks$total)
  
  items <- cont$tracks$items

  tracklist <- c()
  
  numitems <- length(items)
  
  for (i in 1:numitems) {
    tracklist <- c(tracklist, items[[i]]$track$id)
    allTracks[items[[i]]$track$id] <- items[[i]]$track
  }
  trackIDs[[cont$id]] <- tracklist
}

PL_data <- data.frame(ids=ids, name=names, followers=follows, tracks=numtracks,
                      date=rep(as.Date.POSIXct(dates[1]), length(ids)),
                      collab=collaborative, description=description,
                      owner=owner, public=public)
## Write it out:
#write.csv(PL_data, 'PLdata.csv')
## Read it in from Github:
#PL_data <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/PLdata.csv')
head(PL_data,2)
```

```{r}
# Keeping this chunk separate so as to avoid accidentally re-initializing results to empty
PL_tracks <- c()
```

```{r}
for (i in 1:length(resps)) {
  if (i%%100 == 0) {print(i)}
  if (resps[[i]]$status_code == 200) {
    con <- content(resps[[i]])
    if (con$id %in% ids) {
      tracks <- c()
      numitems <- length(con$tracks$items)
      for (j in 1:numitems) {
        tracks <- c(tracks, con$tracks$items[[j]]$track$id)
      }
      PL_tracks[[con$id]] <- tracks
    }
  }
}
# save to dataframe
pldf <- data.frame(PLid=names(PL_tracks))
pldf$PLtracks <- PL_tracks
dim(pldf)
# R won't write a d.frame with a column of lists
pldf$PLtracks <- as.character(pldf$PLtracks)
## Write it out:
#write.csv(pldf, 'PLtracks.csv')
## Read it in from Github later:
# PLtracks <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/PLtracks.csv')
```



```{r}
## see if any of those playlists had greater than 100 tracks, since that is a hidden limit on the GET playlist request
bigIDs <- c()
bigTotals <- c()
for (r in resps) {
  c <- content(resps[[1]])
  if (c$tracks$total > 100) {
    bigIDs <- c(bigIDs, c$id)
    bigTotals <- c(bigTotals, c$tracks$total)
  }
}
length(bigTotals)
```
 OK so that wasn't an issue.  Now let's see about the track info for tracks in playlists.  
 

### Need to deal with the large allTracks list

```{r}
# Keeping this chunk separate so as to avoid accidentally re-initializing results to empty
trackDF <- data.frame(name=c(), released=c(), artist=c(),
                      artistID=c(), albumID=c())
```
```{r}
#loop over all stored track IDs to populate dframe
parse_track <- function(trackID) {
  tr <- allTracks[trackID][[1]]
  released <- tr$release_date
  artist <- tr$artists[[1]]$name
  artID <- tr$artists[[1]]$id
  name <- tr$name
  albID <- tr$id
  return(list(name=name, released=released, artist=artist,
              artistID=artID, albumID=albID)) 
}
tnames <- names(allTracks)
for (i in 1:length(tnames)) {
  if(i%%2000 == 0){print(i)}
  trackDF <- rbind(trackDF, parse_track(tnames[i]))
}
#write.csv(trackDF, 'trackDF.csv')
dim(trackDF)
# clear up 5.7 GB of space here
allTracks <- 0
```

**And since I didn't store the track ID's in the lists/rows, I have to do it all over again.  Might as well add some features, like "popularity" of the song, while at it.**

```{r}
# Keeping this chunk separate so as to avoid accidentally re-initializing results to empty
trackFrame <- data.frame(ids=c(), name=c(), popularity=c(),
                         duration=c(), artistID=c(), artistName=c(),
                         albumID=c(), albumName=c(), released=c())
```
```{r}
auth <- get_auth()
baseUrl <- 'https://api.spotify.com/v1/'
track_endpt <- 'tracks?ids='
comma <- '%2C'
limit <- 50 #Spotify-imposed constraint
```
```{r}
get_tracks_info <- function(IDvec) {
  # This query uses comma-separated arrays of up to 50 track IDs per query
  track_url <- paste0(baseUrl, track_endpt, glue_collapse(IDvec, sep=comma))
  track_resp <- GET(track_url, add_headers(Authorization=auth))
  return(track_resp)
}

alltracks <- tnames
length(alltracks)
# make sure not to dupe a bunch of API calls
length(unique(alltracks))

for (i in 1:ceiling(length(alltracks) / limit)) {
  # monitor progress
  if (i%%100==0) {print(i)}
  
  IDs <- alltracks[(i*limit-limit+1):(i*limit)]
  res <- get_tracks_info(IDs)
  # need to refresh auth code every hour
  if (res$status_code == 401) {
    # print failed query to redo it after dust settles
    print(i)
    print('reset auth key')
    auth <- get_auth()
  }
  if (res$status_code == 200) {
    res <- content(res)$tracks
    for (j in 1:length(res)) {
      t <- res[[j]]
      feats <- list(ids=t$id, name=t$name, popularity=t$popularity, 
                    duration=t$duration_ms, artistID=t$artists[[1]]$id, 
                    artistName=t$artists[[1]]$name,
                    albumID=t$album$id, albumName=t$album$name, 
                    released=t$album$release_date)
      # handle NAs so they don't stop the query loop
      tryCatch(trackFrame <- rbind(trackFrame, feats))
    }
  }
}
# To make the dates comparable, I'm just going to make every album 
## with a year-only release be on the middle day of the year (july 2)
fill_year <- function(str_date){
  if (nchar(str_date) == 4){
    return(paste0(str_date, '-07-02'))
  } else {return(str_date)}
}
fullDates <- as.character(lapply(trackFrame$released, fill_year))
fullDates[1:3]
trackFrame$released <- fullDates
## Write it out:
#write.csv(trackFrame, 'trackFrame.csv')
## Read it back in from Github later:
# trackFrame <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/trackFrame.csv')
dim(trackFrame)
dim(drop_na(trackFrame))

```
```{r}
repo <- get_tracks_info(tnames[1:2])
length(repo$tracks)
names(repo$tracks[[1]])
```


### Get audio features for 100 tracks at a clip  

```{r}
auth <- get_auth()
baseUrl <- 'https://api.spotify.com/v1/'
track_endpt <- 'audio-features?ids='
comma <- '%2C'
limit <- 100  #spotify api constraint

```
```{r}
get_track_feats <- function(IDvec) {
  # This query uses comma-separated arrays of up to 100 track IDs per query
  track_url <- paste0(baseUrl, track_endpt, glue_collapse(IDvec, sep=comma))
  track_resp <- GET(track_url, add_headers(Authorization=auth))
  return(track_resp)
}

dfs <- data.frame()

for (batch in 1:ceiling(length(alltracks) / limit)) {
  # monitor progress
  if (batch%%100==0) {print(batch)}
  # collect batch results into this frame for much quicker performance
  tempframe <- data.frame()
  
  IDs <- alltracks[(batch*limit-limit+1):(batch*limit)]

  res <- get_track_feats(IDs)
  if (res$status_code == 401) {
    # print failed query to redo it after dust settles
    print(batch)
    print('reset auth key')
    auth <- get_auth()
  }
  if (res$status_code == 200) {
    res <- content(res)$audio_features
    for (i in 1:length(res)) {
      feats <- data.frame(res[[i]])
      # handle NAs so they don't stop the query loop
      tryCatch(tempframe <- rbind(tempframe, feats))
    }
    dfs <- rbind(dfs, tempframe)
  }
  
}
#write.csv(dfs, 'songfeats.csv')
## Read it back in later from Github:
# songfeats <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/songfeats.csv' )
```

### possibly the last item to query for is artist info

```{r}
auth <- get_auth()
baseUrl <- 'https://api.spotify.com/v1/'
artist_endpt <- 'artists?ids='
comma <- '%2C'
limit <- 50  #spotify api constraint
```
```{r}
get_artist_detes <- function(IDvec) {
  # This query uses comma-separated arrays of up to 50 artist IDs per query
  artist_url <- paste0(baseUrl, artist_endpt, glue_collapse(IDvec, sep=comma))
  artist_resp <- GET(artist_url, add_headers(Authorization=auth))
  return(artist_resp)
}
```
```{r}
artistDFs <- data.frame()
# Keeping this chunk separate so as to avoid accidentally re-initializing results to empty
allArtists <- unique(trackFrame$artistID)
length(allArtists)
```
```{r}
for (batch in 1:ceiling(length(allArtists) / limit)) {
  # monitor progress
  if (batch%%100==0) {print(batch)}
  # collect batch results into this frame for much quicker performance
  tempframe <- data.frame()
  
  IDs <- allArtists[(batch*limit-limit+1):(batch*limit)]
  
  res <- get_artist_detes(IDs)
  if (res$status_code == 401) {
    # print failed query to redo it after dust settles
    print(batch)
    print('reset auth key')
    auth <- get_auth()
  }
  if (res$status_code == 200) {
    res <- content(res)$artists
    
    for (i in 1:length(res)) {
      artist <- res[[i]]
      artfeats <- list(id=artist$id, name=artist$name,
                       followers=artist$followers$total,
                       popularity=artist$popularity,
                       # collapse and glue lists, but add blank space to avoid NA problems in d.frame creation
                       genres=glue_collapse(c(artist$genres, ' '), sep = ','))
      
      # handle NAs so they don't stop the query loop
      tryCatch(tempframe <- rbind(tempframe, artfeats))
    }
    
    artistDFs <- rbind(artistDFs, tempframe)
  }
  
}
## Write it out:
#write.csv(artistDFs, 'artists.csv')
## Read it back in later from Github:
# artistDFs <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/artists.csv')
```

#### Merge playlist df's to add all the tracks in each PL to all the details about that PL.  

```{r}
PLs <- merge(masterframe, pldf, by.x = 'ID', by.y = 'PLid')
dim(PLs)
names(PLs)
```

#### Now merge again with PL_data d.f, which has the same playlists, but also num_followers and date of the snapshot

```{r}
trimmed_PL_data <- select(PL_data, c(ids, followers, date))
PLs <- merge(PLs, trimmed_PL_data, by.x = 'ID', by.y = 'ids')
dim(PLs)
names(PLs)
## Write it out:
#write_csv(PLs, 'PLs.csv')
## Read it back in from Github later:
# PLs <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/PLs.csv')
```

#### That's great to have all the data in one frame, but at 500MB it's too big to push to github.
##### Since most of it is redundant (~5K playlists total, crossed with ~50 countries each playlist), the countries should be separated out and re-connected in Neo4j, for present purposes.  

```{r}
# so we need a frame with all 5000 playlists and their 50 tracks each, and then another frame for the playlists and their countries.
countryPL <- select(PLs, c(ID, country))
plids <- unique(countryPL$ID)
plidlist <- list()
for (plid in plids){
  plidlist$plid <- c()
}
for (i in 1:nrow(countryPL)){
  plidlist[[countryPL$ID[i]]]<- c(plidlist[[countryPL$ID[i]]], countryPL$country[i])
}
plidlist <- lapply(plidlist, unique)
plidlist <- lapply(plidlist, function(l){glue_collapse(l,sep = '|')})
ids <- names(plidlist)
countries <- c()
for (i in 1:length(ids)) {
  countries <- c(countries, plidlist[[ids[i]]])
}
plidframe <- data.frame('id'=ids, 'countries'=countries)
head(plidframe)
```
#### 3-way merge now, to make the full frame

```{r}
PLs <- merge(pldf, PL_data, by.x = 'PLid', by.y = 'ids')
PLs <- merge(PLs, plidframe, by.x = 'PLid', by.y = 'id')
dim(PLs)
tail(PLs, 2)
#write_csv(PLs, 'PLs.csv')
## Read it back in from Github later:
# PLs <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/PLs.csv')
```
### Neo4j is a little short on the string processing tools necessary to split those vectors of strings into a list of strings.  May need to make each vector into one big string.  

```{r}
collapseTracks <- function(trackvec) {
  str_remove(trackvec, 'c\\(') %>%
    str_remove_all('\\\"') %>%
    str_remove('\\\n\\)') %>%
    str_remove_all('\\s')
}
collapseTracks(PLs$PLtracks[1])
```

```{r}
oneStringTracks <- lapply(PLs$PLtracks, collapseTracks)
PLs$PLtracks <- unlist(oneStringTracks)
#write_csv(PLs, 'PLs.csv')
## Read it back in from Github later:
# PLs <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/PLs.csv')

```

#### Missed attaching the playlist genre labels (the original query seed, with country), so make separate frame to add  

```{r}
PLgenres <- data.frame(distinct(masterframe, ID, genre))
dim(PLgenres)
head(PLgenres, 2)
```
So with 5700 unique playlists, there are 2400 multiple genres

```{r}
#write.csv(PLgenres, 'PLgenres.csv')
## Read it back in from Github later:
# PLgenres <- read_csv('https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/PLgenres.csv')
```

Need to push a country code/name frame to github too

```{r}
codes <- unique(masterframe$country)
cnames <- as.character(lapply(codes, code2country))
countryCodeNames <- data.frame('codes'=codes, 'names'=cnames)
#write.csv(countryCodeNames, 'countryCodeNames.csv')
## Read it back in from Github later:
# countryCodeNames <- read_csv(https://raw.githubusercontent.com/ebhtra/msds-607/main/spotifyNeo4j/countryCodeNames.csv)
```

