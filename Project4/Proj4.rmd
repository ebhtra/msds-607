---
title: "Data607_proj4_email_classifier"
author: "Ethan Haley & Sean Connin"
date: "4/26/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction

The purpose of this project is to develop a complete machine learning pipeline to classify incoming email as either ham or spam. And to both evaluate/critique our model(s) on the basis of its performance. Our approach to this task included substantive steps to process our data, engineer new features (with possible predictive value), and then develop a classification scheme using logistic regression and random forest models.

We employed tidymodels for our model development - which included model parameterization, training, cross-validation, and  testing. We also reviewed our results to distinguish elements of the spam/ham emails that were important to the classification results.  

Our project steps included the following:

1.  Collect relevant folder and and files for processing

2.  Process files and text to facilitate modeling

3.  Build, cross-validate, review results of logistic regression classifier

4.  Build, cross_validate, review results of random forest classifier

5.  Compare results to null model

With that sequence in mind, let's build our libraries.

```{r  results='hide' }
library(tidymodels)
library(magrittr)
library(tidyverse)
library(glue)
library(textrecipes)
library(discrim)
library(tidytext)
library(hardhat)
library(randomForest)
library(ranger)
library(themis)
library(skimr)
```

### 1. Collect relevant folder and and files for processing

Get to folder where you downloaded spam/ham corpus on your computer

```{r warning = 'FALSE'}

getwd()

setwd("~/Data_Science/CUNY/Data 607 Acquisition and Management/Assignments/Proj 4/email")

list.files()

filepath <- c()

for (d in c("spam", "easy_ham", "easy_ham_2", "hard_ham", "spam_2")) {
  for (f in dir(d)) {
    path <- file.path(d, f)
    filepath <- c(filepath, path)
  }
}
filepath[1:3]
```

```{r results='FALSE'}
dir()
```

Collect all email filepaths in one vector.

```{r}
filepath <- c()

for (d in c("spam", "easy_ham", "easy_ham_2", "hard_ham", "spam_2")) {
  for (f in dir(d)) {
    path <- file.path(d, f)
    filepath <- c(filepath, path)
  }
}
filepath[1:3]
```

Make a function to split header and body, store filenames, and classify as spam vs. ham.

```{r}
filepaths <- c()
headers <- c()
bodies <- c()
spams <- c()

header_and_body <- function(filestring) {
  linelist <- read_lines(filestring)
  len <- length(linelist)
  for (line in 1:len) {
    if (linelist[line] == '') {
      header <- glue_collapse(linelist[1:(line-1)], sep='\n')
      body <- glue_collapse(linelist[line:len], sep='\n')
      return(c(filestring, header, body, str_starts(filestring, 's')))
    }
  }
}
#Use the vector of filepaths from the previous chunk
for (fp in filepath) {
  parsed <- header_and_body(fp)
  filepaths <- c(filepaths, parsed[1])
  headers <- c(headers, parsed[2])
  bodies <- c(bodies, parsed[3])
  spams <- c(spams, parsed[4])
}

d.f <- data.frame('filepaths'=filepaths, 'headers'=headers,
                  'bodies'=bodies, 'is_spam'=spams)

#write.csv(d.f, 'headersBodies.csv')
```

### 2. Process files and text to facilitate modeling

Build utility functions to transform raw text into useful features for our model.

```{r}

get_sender <- function(string) {
  # 'John@aol.com'
  str_replace(str_match(string, 'From:? \\S+'), 'From:? ', '')
}
get_angles <- function(string) {
  # How many left angle brackets are there
  str_count(string, '<')
}
get_uppers <- function(string) {
  # How many caps chars
  str_count(string, '[A-Z]')
}
get_lowers <- function(string) {
  # How many lower chars
  str_count(string, '[a-z]')
}
get_return_path <- function(string) {
  # should be similar to get_sender(string)
  str_replace_all(str_match(string, 'Return-Path: <\\S+>'),
              c('Return-Path: <' = '', '>' = ''))
}
get_ips <- function(string) {
  # list of all IP addresses
  str_match_all(string, "[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}")
}
get_subject <- function(string) {
  str_remove(str_match(string, 'Subject: [^\\n]+'), 'Subject: ')
}
get_zs <- function(string) {
  str_count(string, 'z{3,6}')
}
get_ys <- function(string) {
  str_count(string, 'y{3,6}')
}
get_xs <- function(string) {
  str_count(string, 'x{3,6}')
}
get_ws <- function(string) {
  str_count(string, 'w{3,6}')
}
get_domains <- function(string) {
  str_count(string, '\\S@\\S+\\.\\S')
}
has_yahoo_groups <- function(string) {
  str_detect(string, '@yahoogroups')
}
re_subject <- function(string) {
  str_detect(string, '(?i)\\bre: ')
}
has_localhost <- function(ip_list) {
  '127.0.0.1' %in% ip_list[[1]]
}
get_excl <- function(string) {
  str_count(string, '!')
}
has_bad_html <- function(string) {
  is.logical(possibly(read_html, TRUE)(string))
}
get_colors <- function(string) {
  str_count(string, '(?i)colou?r')
}
get_3Ds <- function(string) {
  str_count(string, '(?i)3d')
}
get_fonts <- function(string) {
  str_count(string, '(?i)font')
}
get_sizes <- function(string) {
  str_count(string, '(?i)size')
}
get_aligns <- function(string) {
  str_count(string, '(?i)align')
}
has_sex_words <- function(string) {
  str_detect(tolower(string), 'sex|penis|penile|viagra')
}
```

Apply the functions to add features to the data.frame

```{r}
d.f$bad_header <- unlist(lapply(d.f$headers, has_bad_html))
d.f$bad_body <- unlist(lapply(d.f$bodies, has_bad_html))
d.f$senders <- unlist(lapply(d.f$headers, get_sender))
d.f$subjects <- unlist(lapply(d.f$headers, get_subject))
d.f$noSub <- is.na(d.f$subjects)
d.f$re_subj <- unlist(lapply(d.f$subjects, re_subject))
d.f$headercolors <- unlist(lapply(d.f$headers, get_colors))
d.f$bodycolors <- unlist(lapply(d.f$bodies, get_colors))
d.f$header3Ds <- unlist(lapply(d.f$headers, get_3Ds))
d.f$body3Ds <- unlist(lapply(d.f$bodies, get_3Ds))
d.f$headerfonts <- unlist(lapply(d.f$headers, get_fonts))
d.f$bodyfonts <- unlist(lapply(d.f$bodies, get_fonts))
d.f$headersizes <- unlist(lapply(d.f$headers, get_sizes))
d.f$bodysizes <- unlist(lapply(d.f$bodies, get_sizes))
d.f$headeraligns <- unlist(lapply(d.f$headers, get_aligns))
d.f$bodyaligns <- unlist(lapply(d.f$bodies, get_aligns))
d.f$headerWs <- unlist(lapply(d.f$headers, get_ws))
d.f$headerXs <- unlist(lapply(d.f$headers, get_xs))
d.f$headerYs <- unlist(lapply(d.f$headers, get_ys))
d.f$headerZs <- unlist(lapply(d.f$headers, get_zs))
d.f$bodyWs <- unlist(lapply(d.f$bodies, get_ws))
d.f$bodyXs <- unlist(lapply(d.f$bodies, get_xs))
d.f$bodyYs <- unlist(lapply(d.f$bodies, get_ys))
d.f$bodyZs <- unlist(lapply(d.f$bodies, get_zs))
d.f$headerExcl <- unlist(lapply(d.f$headers, get_excl))
d.f$subjectExcl <- unlist(lapply(d.f$subjects, get_excl))
d.f$headerChars <- unlist(lapply(d.f$headers, possibly(nchar, 1)))
d.f$bodyChars <- unlist(lapply(d.f$bodies, possibly(nchar, 1)))
d.f$yahoos <- unlist(lapply(d.f$headers, has_yahoo_groups))

ipLists <- lapply(d.f$headers, get_ips)
d.f$hasLocals <- unlist(lapply(ipLists, has_localhost))

d.f$domains <- unlist(lapply(d.f$headers, get_domains))
d.f$subjectCAPS <- unlist(lapply(d.f$subjects, get_uppers))
d.f$headerBodyRatio <- d.f$headerChars / d.f$bodyChars
d.f$headerAngles <- unlist(lapply(d.f$headers, get_angles))
d.f$bodyAngles <- unlist(lapply(d.f$bodies, get_angles))
d.f$hAngleRatio <- d.f$headerAngles / d.f$headerChars
d.f$bAngleRatio <- d.f$bodyAngles / d.f$bodyChars
d.f$bodyCAPS <- unlist(lapply(d.f$bodies, get_uppers)) / d.f$bodyChars
d.f$bodyLowers <- unlist(lapply(d.f$bodies, get_lowers)) / d.f$bodyChars
d.f$exclRatio <- unlist(lapply(d.f$bodies, get_excl)) / d.f$bodyChars
d.f$subjectSex <- unlist(lapply(d.f$subjects, has_sex_words))

# Review results

head(d.f, 2)

```

Create a summary of feature statistics and categories.

```{r}
skim(d.f)
```

Group features by type prior to modeling.

```{r}
d.f$is_spam <- as.logical(d.f$is_spam)
cl <- lapply(d.f, class)
numericals <- names(d.f)[cl == 'numeric' | cl == 'integer']
logicals <- names(d.f)[cl == 'logical']
d.f[logicals] <- lapply(d.f[logicals], factor)
characters <- names(d.f)[cl == 'character']
# check status of target column, which has to be factor
is.factor(d.f$is_spam)
```

For numerical features, we recode missing values (NA), which some models refuse to work with.

Confirm factor levels

```{r}

colMeans(d.f[numericals])

# find the NA subjects and replace them with 0
sum(d.f$noSub == T)
sum(is.na(d.f$subjectCAPS))
sum(is.na(d.f$subjectExcl))


d.f$subjectCAPS[d.f$noSub == T] <- 0
d.f$subjectExcl[d.f$noSub == T] <- 0
colMeans(d.f[numericals])

# make sure all factors have at least 2 levels

lapply(d.f[logicals], unique)
```

For subject features we recode so that missing values (NA) = FALSE.

```{r}

d.f$re_subj[is.na(d.f$re_subj)] <- FALSE
d.f$subjectSex[is.na(d.f$subjectSex)] <- FALSE
# and change the subjects themselves too
d.f$subjects[is.na(d.f$subjects)] <- "no subject"
lapply(d.f[logicals], unique)
```

###3. Build, cross-validate, review results of logistic regression classifier

Split dataset into training and testing groups.

```{r}
# set the random seed, for reproducibility, and split the data 80/20.
set.seed(607)

traintest <- initial_split(d.f, prop = .80)
train_data <- training(traintest)
test_data  <- testing(traintest)
```

Make a recipe to scale numerical features for logistic regression.

```{r}
rec <- recipe(is_spam ~ ., data = train_data) %>%
  
  # Keep string features out of the modeling, but keep them around.
  
  update_role(all_of(characters), new_role = "ID") %>%
  step_normalize(all_of(numericals)) %>%
  
  # One-hot encode all logicals except for target
 
  step_dummy(all_of(logicals), -is_spam)
  
# prep() fits the scaler to the training data

scaler <- prep(rec, training = train_data)

# and bake() transforms all data using the statistics learned by prep()

scaled_train <- bake(scaler, train_data)

scaled_test <- bake(scaler, test_data)
```

Now the scaled data can be used to train models. Or at least that shows conceptually how the scaler will be fit to and transform the data. But instead of exiting the pipeline so soon, recipes can fit inside of a larger pipeline called a `workflow()`, which manages all the scaling steps as part of a model parameter fitting and prediction process. It just needs the data, the recipe, and a model.

One natural model choice to start with might be a logistic regression classifier, such as `logistic_reg()` from the `parsnip` package.

```{r}
lr_mod <- logistic_reg() %>%
  set_engine('glm') # barebones log_reg

spam_workflow <- workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(rec)

spam_workflow
```

Fit the logistic model to the training set.

```{r warning=F}
spam_fit <- spam_workflow %>%
  fit(data = train_data)
```

See which features had most influence making email look like HAM.

```{r}
spam_fit %>% 
  pull_workflow_fit() %>% 
  tidy() %>%
  # estimate == coefficient
  arrange(estimate) %>%
  select(c(term, estimate, p.value))
```

Results: The YahooGroups coefficient has too high of a p-value to be worth anything, but after that, the biggest identifiers of legit email (Are you listening, all you spammers out there?) are emails that have subjects showing "RE:" in them, email bodies with websites in them ('bodyWs'), emails with a high ratio of lower case letters in the email, with a lot of angle brackets in the header, and with a localhost IP address (127.0.0.1) in the header.

Now we assess the most influential SPAM indicators:

```{r}
spam_fit %>% 
  pull_workflow_fit() %>% 
  tidy() %>%
  arrange(-estimate) %>%
  select(c(term, estimate, p.value))
```

Results: A lot of capital letters and exclamation points in the email body won't help it get past the spam filter, unsurprisingly, nor will mentioning words like viagra and sex organs. If there are many fonts or other html markup, if a subject field is left out, or if many domains are in the header (think email blasts), those are other good indicators of spamminess.

Now we evaluate how well the trained model performs on the test data predictions.

Note: We can set the logistic regression model to output its predictions in probabilities, rather than just binary 'yes'/'no' predictions, and that way we can look at the area under the ROC as a means of evaluating the model over all choices of threshold, rather than just .50.

```{r}
spam_pred <- 
  predict(spam_fit, test_data, type = "prob") %>% 
  bind_cols(test_data %>% select(is_spam)) # bound the true values for visual inspection of predictions

spam_pred
```

Plot our model roc_curve

```{r}
spam_pred %>% 
  roc_curve(truth = is_spam, .pred_FALSE) %>% 
  autoplot()
```

```{r}
spam_pred %>% 
  roc_auc(truth = is_spam, .pred_FALSE)
```

We have a reasonable ROC-AUC for a starter model. We should also assess our model's accuracy?

```{r}
accu <- sum((spam_pred$.pred_TRUE > 0.5) == (spam_pred$is_spam)) / nrow(spam_pred)
accu
```

Note: if you change the threshold from the default 0.5 used above, the accuracy improves slightly as you lower the classification threshold, down to 0.4 and even 0.3. However, there are reasons to avoid this step, if your goal is to measure the predictive strength of a model.

For example, changing model parameters after seeing the test results could improve model accuracy by causing your filter to send more emails, both spam and ham, to the spam folder. 

While in some classification applications, such as perhaps medical diagnostics, improving predictive accuracy at the expense of more false positives might be a good tradeoff, it's hard to imagine someone wanting to see less spam in her inbox at the expense of having to constantly check her spam folder to make sure she didn't miss important ham.

Our model's accuracy (85.7%) is acceptable given that ~32% of the training data is spam. It's worth noting that the ratio of model inputs (spam & ham email) are unbalanced such that we are undersampling the majority (ham) group. 

We can rebalance our training/test data to avoid this. The `themis` package is one of many that can perform this duty for us.

```{r warning=F}
# Modify the existing recipe and then update the workflow with it
email_recipe <- rec %>%
    themis::step_downsample(is_spam, under_ratio = 1) # 1 is equal spam/ham
spam_workflow <- spam_workflow %>%
  update_recipe(email_recipe)
# Re-train the workflow model
spam_fit <- spam_workflow %>%
  fit(data = train_data)
```

We check our model's accuracy after rebalancing our data inputs.

```{r}
spam_pred <- 
  predict(spam_fit, test_data, type = "prob") %>% 
  bind_cols(test_data %>% select(is_spam))
accu <- sum((spam_pred$.pred_TRUE > 0.5) == (spam_pred$is_spam)) / nrow(spam_pred)
glue('The test accuracy after fitting the same model on a balanced training set is 
{round(100 * accu, 1)}%, vs 85.7% from imbalanced training.')
```

Now that we've gotten a glimpse of what a basic logistic regression model can learn from some roughly handcrafted features, it's tempting to find an even more powerful modern model (e.g., random forest), feed it more data, in the form of email text, and see what sorts of patterns and relationships it can find and use to make predictions. So why wait?

First, a look at what sorts of words the language processor will be dealing with.

```{r}
# review data in tidy form
tidy<-d.f %>%
    unnest_tokens(word, bodies) %>%
    group_by(word) %>%
    filter(n()>20) %>%
        ungroup()
tidy %>%
  count(is_spam, word, sort = TRUE) %>%
  anti_join(get_stopwords()) %>%
  group_by(is_spam) %>%
  top_n(20) %>%
  ungroup() %>%
  ggplot(aes(reorder_within(word, n, is_spam), n,
    fill = is_spam
  )) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~is_spam, scales = "free") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(
    x = NULL, y = "Word count",
    title = "Most frequent words")
```

##4.  Build, cross_validate, review results of random forest classifier

Our first step in constructing a random forest model is to update the recipe to calculate and select important email message tokens.

```{r}
# reminder: "bodies" is a feature column in the d.frame, which holds the email text
email_recipe <- email_recipe %>%
    step_tokenize(bodies) %>%
    step_tokenfilter(bodies, max_tokens = 1e3) %>%
    step_tfidf(bodies) 
```

Initialize a random forest model to plug into the workflow

```{r}
rf<- rand_forest(trees = 200, mtry = 20, min_n = 3) %>%
    set_engine("ranger") %>%
    set_mode("classification") 
```

Make cross-validation folds

```{r}
email_folds <- vfold_cv(data = train_data, strata = is_spam, v = 5)
```

Update the model workflow to incorporate these changes

Note: this would be an appropriate place to build a sparse matrix for the data to facilitate model speed. However, we were unable to coerce a matrix into the df. 

```{r}
spam_workflow<- spam_workflow %>%         
    update_recipe(email_recipe) %>%
    update_model(rf)

rf_model_fit <- fit_resamples(
    spam_workflow,
    email_folds,
    metrics = metric_set(roc_auc, accuracy),
    control = control_resamples(save_pred=TRUE))

collect_metrics(rf_model_fit) 
```

While it's possible that the model has overfit to produce these highly accurate numbers, random forest models are relatively resilient to over-fitting, by limiting the pool of features to choose from for each fit (we set the `mtry` argument to 20 possible features each split, when initializing the rf model), and by using many different trees (we used 200 here). 

Let's train the model with the 5 folds and see how it performs on the test data.

```{r}


# Use last_fit to fit to the full (downsampled) training set and then evaluate the full test set

rf_final<-
    spam_workflow %>%
    last_fit(traintest)

cmat <- rf_final %>%
    collect_predictions() %>%
    conf_mat(truth = is_spam, estimate = .pred_class)
summary(cmat)
```

The accuracy is actually a little higher now, perhaps from having 25% more data to train on. Let's see what sorts of errors it's still making.

```{r}
autoplot(cmat, type = "heatmap")
```

The results indicate a high accuracy level, but if this model were used in the real world, we might want to raise the classification threshold from 0.5 in order to reduce the 23 false positives (lower left) at the expense of the 3 false negatives (upper right).

Lets evaluate our model results with this in mind. 

```{r}
preds <- rf_final$.predictions

false_negs <- preds[[1]] %>%
  filter(.pred_class == FALSE) %>%
  filter(is_spam == TRUE)

false_pos <- preds[[1]] %>%
  filter(.pred_class == TRUE) %>%
  filter(is_spam == FALSE)

```

```{r}
false_pos
```

```{r}
hard_hams <- c()
for (row in 1:length(d.f$filepaths)) {
  if (str_starts(d.f$filepaths[row], 'hard')) {
    hard_hams <- c(hard_hams, row)
  }
}
glue('Hard hams start at row {min(hard_hams)} and go to row {max(hard_hams)}.  
There are {length(hard_hams)} of them.')
```

So 20 of the 26 false positives were from the "hard ham" folder. If you look at one of these false positives (the last one, for example, since it had a 73.5% prediction of being spam, which was approximately median for this tough group), you can see the difficulty.

```{r}
d.f$bodies[4650]
```

How about those 3 sneaky spams that got through the defenses?

```{r}
false_negs
```

Two of them had a pretty even chance of being filtered out, but let's look at that one that was only given a 21% chance of being spam.

```{r}
d.f$bodies[5816]
```

Perhaps one person's ham is another's spam. As a final check on our Random Forest model results we check output from a null model

```{r}
#Evaluate null model results as a check on ourselves
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")
null_wf <- workflow() %>%
  add_recipe(email_recipe) %>%
  add_model(null_classification)
null_model_fit <- fit_resamples(
    null_wf,
    email_folds,
    metrics = metric_set(roc_auc, accuracy),
    control = control_resamples(save_pred=TRUE))
collect_metrics(null_model_fit)  #--> roc_auc .50, accuracy = .68
```

The null model provides us with an roc_auc and accuracy that we might expect by guessing the type of email (spam vs. ham). Comparing to our null results, it's clear that our logistic and random forest models have performed well. 
