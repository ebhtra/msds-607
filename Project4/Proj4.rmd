---
title: "Hormail"
author: "Sean Connin & Ethan Haley"
date: "4/26/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidymodels)
library(magrittr)
library(tidyverse)
library(glue)
library(textrecipes)
library(discrim)
library(tidytext)
library(hardhat)
library(randomForest)
library(ranger)
library(themis)
library(skimr)
```

**Get to folder where you downloaded spam/ham corpus on your computer**

```{r}
getwd()
```

```{r}
dir()
```

**Collect all email filepaths in one vector**

```{r}
filepath <- c()

for (d in c("spam", "easy_ham", "easy_ham_2", "hard_ham", "spam_2")) {
  for (f in dir(d)) {
    path <- file.path(d, f)
    filepath <- c(filepath, path)
  }
}
filepath[1:3]
```

**Make a function to split header and body, store filenames, and classify as spam/ham**

```{r}
filepaths <- c()
headers <- c()
bodies <- c()
spams <- c()

header_and_body <- function(filestring) {
  linelist <- read_lines(filestring)
  len <- length(linelist)
  for (line in 1:len) {
    if (linelist[line] == '') {
      header <- glue_collapse(linelist[1:(line-1)], sep='\n')
      body <- glue_collapse(linelist[line:len], sep='\n')
      return(c(filestring, header, body, str_starts(filestring, 's')))
    }
  }
}
#Use the vector of filepaths from the previous chunk
for (fp in filepath) {
  parsed <- header_and_body(fp)
  filepaths <- c(filepaths, parsed[1])
  headers <- c(headers, parsed[2])
  bodies <- c(bodies, parsed[3])
  spams <- c(spams, parsed[4])
}

d.f <- data.frame('filepaths'=filepaths, 'headers'=headers,
                  'bodies'=bodies, 'is_spam'=spams)

#write.csv(d.f, 'headersBodies.csv')
```

### Some utility functions to transform raw text into useful features for ML  

```{r}

get_sender <- function(string) {
  # 'John@aol.com'
  str_replace(str_match(string, 'From:? \\S+'), 'From:? ', '')
}
get_angles <- function(string) {
  # How many left angle brackets are there
  str_count(string, '<')
}
get_uppers <- function(string) {
  # How many caps chars
  str_count(string, '[A-Z]')
}
get_lowers <- function(string) {
  # How many lower chars
  str_count(string, '[a-z]')
}
get_return_path <- function(string) {
  # should be similar to get_sender(string)
  str_replace_all(str_match(string, 'Return-Path: <\\S+>'),
              c('Return-Path: <' = '', '>' = ''))
}
get_ips <- function(string) {
  # list of all IP addresses
  str_match_all(string, "[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}")
}
get_subject <- function(string) {
  str_remove(str_match(string, 'Subject: [^\\n]+'), 'Subject: ')
}
get_zs <- function(string) {
  str_count(string, 'z{3,6}')
}
get_ys <- function(string) {
  str_count(string, 'y{3,6}')
}
get_xs <- function(string) {
  str_count(string, 'x{3,6}')
}
get_ws <- function(string) {
  str_count(string, 'w{3,6}')
}
get_domains <- function(string) {
  str_count(string, '\\S@\\S+\\.\\S')
}
has_yahoo_groups <- function(string) {
  str_detect(string, '@yahoogroups')
}
re_subject <- function(string) {
  str_detect(string, '(?i)\\bre: ')
}
has_localhost <- function(ip_list) {
  '127.0.0.1' %in% ip_list[[1]]
}
get_excl <- function(string) {
  str_count(string, '!')
}
has_bad_html <- function(string) {
  is.logical(possibly(read_html, TRUE)(string))
}
get_colors <- function(string) {
  str_count(string, '(?i)colou?r')
}
get_3Ds <- function(string) {
  str_count(string, '(?i)3d')
}
get_fonts <- function(string) {
  str_count(string, '(?i)font')
}
get_sizes <- function(string) {
  str_count(string, '(?i)size')
}
get_aligns <- function(string) {
  str_count(string, '(?i)align')
}
has_sex_words <- function(string) {
  str_detect(tolower(string), 'sex|penis|penile|viagra')
}
```

### Apply the above functions to add features to the data.frame

```{r}
d.f$bad_header <- unlist(lapply(d.f$headers, has_bad_html))
d.f$bad_body <- unlist(lapply(d.f$bodies, has_bad_html))
d.f$senders <- unlist(lapply(d.f$headers, get_sender))
d.f$subjects <- unlist(lapply(d.f$headers, get_subject))
d.f$noSub <- is.na(d.f$subjects)
d.f$re_subj <- unlist(lapply(d.f$subjects, re_subject))
d.f$headercolors <- unlist(lapply(d.f$headers, get_colors))
d.f$bodycolors <- unlist(lapply(d.f$bodies, get_colors))
d.f$header3Ds <- unlist(lapply(d.f$headers, get_3Ds))
d.f$body3Ds <- unlist(lapply(d.f$bodies, get_3Ds))
d.f$headerfonts <- unlist(lapply(d.f$headers, get_fonts))
d.f$bodyfonts <- unlist(lapply(d.f$bodies, get_fonts))
d.f$headersizes <- unlist(lapply(d.f$headers, get_sizes))
d.f$bodysizes <- unlist(lapply(d.f$bodies, get_sizes))
d.f$headeraligns <- unlist(lapply(d.f$headers, get_aligns))
d.f$bodyaligns <- unlist(lapply(d.f$bodies, get_aligns))
d.f$headerWs <- unlist(lapply(d.f$headers, get_ws))
d.f$headerXs <- unlist(lapply(d.f$headers, get_xs))
d.f$headerYs <- unlist(lapply(d.f$headers, get_ys))
d.f$headerZs <- unlist(lapply(d.f$headers, get_zs))
d.f$bodyWs <- unlist(lapply(d.f$bodies, get_ws))
d.f$bodyXs <- unlist(lapply(d.f$bodies, get_xs))
d.f$bodyYs <- unlist(lapply(d.f$bodies, get_ys))
d.f$bodyZs <- unlist(lapply(d.f$bodies, get_zs))
d.f$headerExcl <- unlist(lapply(d.f$headers, get_excl))
d.f$subjectExcl <- unlist(lapply(d.f$subjects, get_excl))
d.f$headerChars <- unlist(lapply(d.f$headers, possibly(nchar, 1)))
d.f$bodyChars <- unlist(lapply(d.f$bodies, possibly(nchar, 1)))
d.f$yahoos <- unlist(lapply(d.f$headers, has_yahoo_groups))

ipLists <- lapply(d.f$headers, get_ips)
d.f$hasLocals <- unlist(lapply(ipLists, has_localhost))

d.f$domains <- unlist(lapply(d.f$headers, get_domains))
d.f$subjectCAPS <- unlist(lapply(d.f$subjects, get_uppers))
d.f$headerBodyRatio <- d.f$headerChars / d.f$bodyChars
d.f$headerAngles <- unlist(lapply(d.f$headers, get_angles))
d.f$bodyAngles <- unlist(lapply(d.f$bodies, get_angles))
d.f$hAngleRatio <- d.f$headerAngles / d.f$headerChars
d.f$bAngleRatio <- d.f$bodyAngles / d.f$bodyChars
d.f$bodyCAPS <- unlist(lapply(d.f$bodies, get_uppers)) / d.f$bodyChars
d.f$bodyLowers <- unlist(lapply(d.f$bodies, get_lowers)) / d.f$bodyChars
d.f$exclRatio <- unlist(lapply(d.f$bodies, get_excl)) / d.f$bodyChars
d.f$subjectSex <- unlist(lapply(d.f$subjects, has_sex_words))


```

```{r}
head(d.f, 2)
```
**Check how the features are categorized**

```{r}
skim(d.f)
```

**Group features by type**

```{r}
d.f$is_spam <- as.logical(d.f$is_spam)
cl <- lapply(d.f, class)
numericals <- names(d.f)[cl == 'numeric' | cl == 'integer']
logicals <- names(d.f)[cl == 'logical']
d.f[logicals] <- lapply(d.f[logicals], factor)
characters <- names(d.f)[cl == 'character']
# check status of target column, which has to be factor
is.factor(d.f$is_spam)
```

**Take care of NAs, which some models refuse to work with**

```{r}
colMeans(d.f[numericals])
```

```{r}
# find the NA subjects and replace them with 0
sum(d.f$noSub == T)
sum(is.na(d.f$subjectCAPS))
sum(is.na(d.f$subjectExcl))
```

```{r}
d.f$subjectCAPS[d.f$noSub == T] <- 0
d.f$subjectExcl[d.f$noSub == T] <- 0
colMeans(d.f[numericals])
```

```{r}
# make sure all factors have at least 2 levels
lapply(d.f[logicals], unique)
```

**Should probably make those NA's be FALSE in the subject features**

```{r}
d.f$re_subj[is.na(d.f$re_subj)] <- FALSE
d.f$subjectSex[is.na(d.f$subjectSex)] <- FALSE
# and change the subjects themselves too
d.f$subjects[is.na(d.f$subjects)] <- "no subject"
lapply(d.f[logicals], unique)
```

### Split into training and testing groups

```{r}
# set the random seed, for reproducibility, and split the data 80/20.
set.seed(607)

traintest <- initial_split(d.f, prop = .80)
train_data <- training(traintest)
test_data  <- testing(traintest)
```

**Make a recipe to scale numerics for logistic regression**

```{r}
rec <- recipe(is_spam ~ ., data = train_data) %>%
  # Keep string features out of the modeling, but keep them around.
  update_role(all_of(characters), new_role = "ID") %>%
  step_normalize(all_of(numericals)) %>%
  # One-hot encode all logicals except for target
  step_dummy(all_of(logicals), -is_spam)
# prep() fits the scaler to the training data
scaler <- prep(rec, training = train_data)
# and bake() transforms all data using the statistics learned by prep()
scaled_train <- bake(scaler, train_data)
scaled_test <- bake(scaler, test_data)
```

Now the scaled data can be used to train models.  Or at least that shows conceptually how the scaler will be fit to and transform the data. But instead of exiting the pipeline so soon, recipes can fit inside of a larger pipeline called a `workflow()`, which manages all the scaling steps as part of a model parameter fitting and prediction process.  It just needs the data, the recipe, and a model.

One natural model choice to start with might be a logistic regression classifier, such as `logistic_reg()` from the `parsnip` package. 

```{r}
lr_mod <- logistic_reg() %>%
  set_engine('glm') # barebones log_reg

spam_workflow <- workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(rec)

spam_workflow
```

Fit the model to the training set.  

```{r warning=F}
spam_fit <- spam_workflow %>%
  fit(data = train_data)
```

### See which features had most influence making email look like HAM

```{r}
spam_fit %>% 
  pull_workflow_fit() %>% 
  tidy() %>%
  # estimate == coefficient
  arrange(estimate) %>%
  select(c(term, estimate, p.value))
```
**The YahooGroups coefficient has too high of a p-value to be worth anything, but after that, the biggest identifiers of legit email (Are you listening, all you spammers out there?) are emails that have subjects showing "RE:" in them, email bodies with websites in them ('bodyWs'), emails with a high ratio of lower case letters in the email, with a lot of angle brackets in the header, and with a localhost IP address (127.0.0.1) in the header.**  

### Now for the highest SPAM indicators:

```{r}
spam_fit %>% 
  pull_workflow_fit() %>% 
  tidy() %>%
  arrange(-estimate) %>%
  select(c(term, estimate, p.value))
```
**A lot of capital letters and exclamation points in the email body won't help it get past the spam filter, unsurprisingly, nor will mentioning words like viagra and sex organs.  If there are many fonts or other html markup, if a subject field is left out, or if many domains are in the header (think email blasts), those are other good indicators of spamminess.**  

### Take a look at how well the trained model performs on the test data predictions  

**We can set the logistic regression model to output its predictions in probabilities, rather than just binary 'yes'/'no' predictions, and that way we can look at the area under the ROC as a means of evaluating the model over all choices of threshold, rather than just .50.**

```{r}
spam_pred <- 
  predict(spam_fit, test_data, type = "prob") %>% 
  bind_cols(test_data %>% select(is_spam))
  # bound the true values for visual inspection of predictions
spam_pred
```

```{r}
spam_pred %>% 
  roc_curve(truth = is_spam, .pred_FALSE) %>% 
  autoplot()
```

```{r}
spam_pred %>% 
  roc_auc(truth = is_spam, .pred_FALSE)
```
**Looks like a pretty reasonable ROC-AUC, for a starter model.  How about the accuracy?**  

```{r}
accu <- sum((spam_pred$.pred_TRUE > 0.5) == (spam_pred$is_spam)) / nrow(spam_pred)
accu
```

**Incidentally, but importantly, if you change the threshold from the default 0.5 used above, the accuracy improves slightly as you lower the classification threshold, down to 0.4 and even 0.3.   But this is the wrong thing to do, for two reasons:**  
 - You shouldn't tweak parameters after seeing test results, if your goal is to measure the PREdictive strength of a model.
 - You would be improving accuracy by causing your filter to send more emails, both spam and ham, to the spam folder.  While in some classification applications, such as perhaps medical diagnostics, improving predictive accuracy at the expense of more false positives might be a good tradeoff, it's hard to imagine someone wanting to see less spam in her inbox at the expense of having to constantly check her spam folder to make sure she didn't miss important ham.

**85.7%  accuracy isn't too bad, since about 32% of the training data is spam, meaning that a useless spam filter that let every email through would score 68% accuracy.  This spam/ham imbalance brings up another idea, about training the model on a balanced spam/ham dataset, by undersampling the majority (ham) group.**  

The `themis` package is one of many that can perform this duty for us. 

```{r warning=F}
# Modify the existing recipe and then update the workflow with it
email_recipe <- rec %>%
    themis::step_downsample(is_spam, under_ratio = 1) # 1 is equal spam/ham
spam_workflow <- spam_workflow %>%
  update_recipe(email_recipe)
# Re-train the workflow model
spam_fit <- spam_workflow %>%
  fit(data = train_data)
```
```{r}
spam_pred <- 
  predict(spam_fit, test_data, type = "prob") %>% 
  bind_cols(test_data %>% select(is_spam))
accu <- sum((spam_pred$.pred_TRUE > 0.5) == (spam_pred$is_spam)) / nrow(spam_pred)
glue('The test accuracy after fitting the same model on a balanced training set is 
{round(100 * accu, 1)}%, vs 85.7% from imbalanced training.')
```

**Now that we've gotten a glimpse of what a barebones regression model can learn from some roughly handcrafted features, it's tempting to find an even more powerful modern model, feed it more data, in the form of email text, and see what sorts of patterns and relationships it can find and use to make predictions.  So why wait?**  

### Update the recipe to calculate and select important email message tokens

```{r}
# reminder: "bodies" is a feature column in the d.frame, which holds the email text
email_recipe <- email_recipe %>%
    step_tokenize(bodies) %>%
    step_tokenfilter(bodies, max_tokens = 1e3) %>%
    step_tfidf(bodies) 
```

**Initialize a random forest model to plug into the workflow**

```{r}
rf<- rand_forest(trees = 200, mtry = 20, min_n = 3) %>%
    set_engine("ranger") %>%
    set_mode("classification") 
```

**Make cross-validation folds**  

```{r}
email_folds <- vfold_cv(data = train_data, strata = is_spam, v = 5)
```

**Update workflow**

```{r}
spam_workflow<- spam_workflow %>%         
    update_recipe(email_recipe) %>%
    update_model(rf)
#Fit model (Random Forest) with cross validation. #this code comes from https://rpubs.com/shampjeff/tidy_spam_clf
#note: was unable to coerce sparse matrix into df with random forest model. Not clear why. 
rf_model_fit <- fit_resamples(
    spam_workflow,
    email_folds,
    metrics = metric_set(roc_auc, accuracy),
    control = control_resamples(save_pred=TRUE))
collect_metrics(rf_model_fit)  #roc_auc = .997, accuracy = .977
```
**While it's possible that the model has overfit to produce these highly accurate numbers, random forests are relatively resilient to overfitting, by limiting the pool of features to choose from for each fit (we set the `mtry` argument to 20 possible features each split, when initializing the rf model), and by using many different trees (we used 200 here).  But let's train the model on all 5 folds now and see how it does on the held out test data.**  

```{r}
# Use last_fit to fit to the full (downsampled) training set and then evaluate the full test set
rf_final<-
    spam_workflow %>%
    last_fit(traintest) %>%
    collect_predictions() %>%
    conf_mat(truth = is_spam, estimate = .pred_class)
summary(rf_final)
```
**The accuracy is actually a little higher now, perhaps from having 25% more data to train on.  Let's see what sorts of errors it's still making.**  

```{r}
autoplot(rf_final, type = "heatmap")
```

**Those results are satisfying from an accuracy standpoint, but if this model were used in the real world, we might want to raise the classification threshold from 0.5 in order to reduce the 23 false positives (lower left) at the expense of the 3 false negatives (upper right).** 


```{r}
#Evaluate null model results as a check on ourselves
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")
null_wf <- workflow() %>%
  add_recipe(email_recipe) %>%
  add_model(null_classification)
null_model_fit <- fit_resamples(
    null_wf,
    email_folds,
    metrics = metric_set(roc_auc, accuracy),
    control = control_resamples(save_pred=TRUE))
collect_metrics(null_model_fit)  #--> roc_auc .50, accuracy = .68
```

